{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a0bb0d-51d4-4d6c-88bf-0a923932b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (2.19.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python numpy pandas tensorflow scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd37ec0-158b-44ac-a847-7365340ffd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing CNN dataset (cropping bubbles)...\n",
      "Step 2: Building CNN...\n",
      "Step 3: Training CNN...\n",
      "Found 304 images belonging to 5 classes.\n",
      "Found 73 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 133ms/step - accuracy: 0.2899 - loss: 1.5014 - val_accuracy: 0.3699 - val_loss: 1.3558\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.3416 - loss: 1.3679 - val_accuracy: 0.3699 - val_loss: 1.3385\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.3736 - loss: 1.4112 - val_accuracy: 0.3699 - val_loss: 1.3331\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.3634 - loss: 1.4430 - val_accuracy: 0.3699 - val_loss: 1.3182\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.3632 - loss: 1.3753 - val_accuracy: 0.3699 - val_loss: 1.3479\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.3437 - loss: 1.3658 - val_accuracy: 0.3699 - val_loss: 1.3192\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.3881 - loss: 1.3098 - val_accuracy: 0.3699 - val_loss: 1.3255\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.3051 - loss: 1.4341 - val_accuracy: 0.3699 - val_loss: 1.3314\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.3441 - loss: 1.3522 - val_accuracy: 0.3699 - val_loss: 1.3228\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.3214 - loss: 1.3751 - val_accuracy: 0.3699 - val_loss: 1.3263\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.3878 - loss: 1.3478 - val_accuracy: 0.3699 - val_loss: 1.3194\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.3656 - loss: 1.3845 - val_accuracy: 0.3836 - val_loss: 1.3300\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.3776 - loss: 1.3395 - val_accuracy: 0.3836 - val_loss: 1.3263\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.3713 - loss: 1.3358 - val_accuracy: 0.3562 - val_loss: 1.3316\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.4195 - loss: 1.2477 - val_accuracy: 0.3562 - val_loss: 1.3551\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.3934 - loss: 1.3511 - val_accuracy: 0.3699 - val_loss: 1.3294\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.4737 - loss: 1.2623 - val_accuracy: 0.3699 - val_loss: 1.3439\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.4453 - loss: 1.3016 - val_accuracy: 0.3836 - val_loss: 1.3346\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.4208 - loss: 1.2402 - val_accuracy: 0.3836 - val_loss: 1.3264\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.5051 - loss: 1.2326 - val_accuracy: 0.3699 - val_loss: 1.3223\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.4668 - loss: 1.1602 - val_accuracy: 0.3562 - val_loss: 1.3382\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.4969 - loss: 1.2152 - val_accuracy: 0.3836 - val_loss: 1.3213\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.4561 - loss: 1.1978 - val_accuracy: 0.3699 - val_loss: 1.3419\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.4650 - loss: 1.2462 - val_accuracy: 0.3699 - val_loss: 1.3351\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.5390 - loss: 1.1527 - val_accuracy: 0.3699 - val_loss: 1.3542\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.4594 - loss: 1.1754 - val_accuracy: 0.3562 - val_loss: 1.3456\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5694 - loss: 1.0614 - val_accuracy: 0.2877 - val_loss: 1.3800\n",
      "Epoch 28/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.5437 - loss: 1.1404 - val_accuracy: 0.3836 - val_loss: 1.3800\n",
      "Epoch 29/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.5565 - loss: 1.0843 - val_accuracy: 0.3699 - val_loss: 1.3881\n",
      "Epoch 30/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5444 - loss: 1.0669 - val_accuracy: 0.3836 - val_loss: 1.4285\n",
      "Epoch 31/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.5494 - loss: 1.0539 - val_accuracy: 0.3973 - val_loss: 1.3942\n",
      "Epoch 32/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.5492 - loss: 1.0326 - val_accuracy: 0.3425 - val_loss: 1.3954\n",
      "Epoch 33/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.5562 - loss: 1.0310 - val_accuracy: 0.3425 - val_loss: 1.4134\n",
      "Epoch 34/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.5592 - loss: 1.0661 - val_accuracy: 0.3288 - val_loss: 1.4405\n",
      "Epoch 35/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.5604 - loss: 1.0411 - val_accuracy: 0.2740 - val_loss: 1.4532\n",
      "Epoch 36/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.5701 - loss: 1.0102 - val_accuracy: 0.3562 - val_loss: 1.5130\n",
      "Epoch 37/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6108 - loss: 0.9315 - val_accuracy: 0.2740 - val_loss: 1.5184\n",
      "Epoch 38/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5982 - loss: 0.9253 - val_accuracy: 0.3562 - val_loss: 1.4629\n",
      "Epoch 39/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.6179 - loss: 0.9898 - val_accuracy: 0.3014 - val_loss: 1.4814\n",
      "Epoch 40/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.6329 - loss: 0.8971 - val_accuracy: 0.3014 - val_loss: 1.5915\n",
      "Epoch 41/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.5948 - loss: 0.9475 - val_accuracy: 0.2877 - val_loss: 1.5632\n",
      "Epoch 42/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.6739 - loss: 0.8126 - val_accuracy: 0.3288 - val_loss: 1.6351\n",
      "Epoch 43/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6388 - loss: 0.8703 - val_accuracy: 0.3425 - val_loss: 1.7174\n",
      "Epoch 44/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6396 - loss: 0.8186 - val_accuracy: 0.3151 - val_loss: 1.6676\n",
      "Epoch 45/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.6800 - loss: 0.7868 - val_accuracy: 0.3288 - val_loss: 1.7550\n",
      "Epoch 46/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.6337 - loss: 0.8230 - val_accuracy: 0.2877 - val_loss: 1.7186\n",
      "Epoch 47/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7015 - loss: 0.7912 - val_accuracy: 0.2877 - val_loss: 1.7353\n",
      "Epoch 48/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.7176 - loss: 0.6928 - val_accuracy: 0.2877 - val_loss: 1.8913\n",
      "Epoch 49/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.6899 - loss: 0.8046 - val_accuracy: 0.3151 - val_loss: 1.8394\n",
      "Epoch 50/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.6805 - loss: 0.7271 - val_accuracy: 0.2877 - val_loss: 1.8752\n",
      "Step 4: Predicting & scoring all sheets...\n",
      "Img1.jpeg (Set A) -> score: 0\n",
      "Img16.jpeg (Set A) -> score: 0\n",
      "Img17.jpeg (Set A) -> score: 0\n",
      "Img18.jpeg (Set A) -> score: 0\n",
      "Img19.jpeg (Set A) -> score: 21\n",
      "Img2.jpeg (Set A) -> score: 1\n",
      "Img20.jpeg (Set A) -> score: 15\n",
      "Img3.jpeg (Set A) -> score: 0\n",
      "Img4.jpeg (Set A) -> score: 0\n",
      "Img5.jpeg (Set A) -> score: 1\n",
      "Img6.jpeg (Set A) -> score: 1\n",
      "Img7.jpeg (Set A) -> score: 0\n",
      "Img8.jpeg (Set A) -> score: 59\n",
      "Img10.jpeg (Set B) -> score: 0\n",
      "Img11.jpeg (Set B) -> score: 36\n",
      "Img12.jpeg (Set B) -> score: 0\n",
      "Img13.jpeg (Set B) -> score: 4\n",
      "Img14.jpeg (Set B) -> score: 0\n",
      "Img15.jpeg (Set B) -> score: 46\n",
      "Img21.jpeg (Set B) -> score: 0\n",
      "Img22.jpeg (Set B) -> score: 0\n",
      "Img23.jpeg (Set B) -> score: 1\n",
      "Img9.jpeg (Set B) -> score: 0\n",
      "Saved CNN-based results to omr_cnn_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CNN-based OMR Scoring Pipeline\n",
    "# ------------------------------\n",
    "\n",
    "# REQUIREMENTS: pip install opencv-python numpy pandas tensorflow scikit-learn openpyxl\n",
    "\n",
    "import os, cv2, numpy as np, pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "dataset_root = \".\"            # folder containing SetA/SetB\n",
    "excel_file   = \"Key (Set A and B).xlsx\"\n",
    "IMG_SIZE = 48\n",
    "CROP_DIR = \"bubble_crops\"\n",
    "NUM_QUESTIONS = 100\n",
    "classes = [\"A\",\"B\",\"C\",\"D\",\"BLANK\"]\n",
    "\n",
    "os.makedirs(CROP_DIR, exist_ok=True)\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(CROP_DIR, cls), exist_ok=True)\n",
    "\n",
    "# ---------- HELPER FUNCTIONS ----------\n",
    "\n",
    "def warp_sheet(img, dst_size=(1200,1700)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    edged = cv2.Canny(blur, 50, 150)\n",
    "    cnts, _ = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:15]\n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02*peri, True)\n",
    "        if len(approx)==4:\n",
    "            pts = approx.reshape(4,2).astype(\"float32\")\n",
    "            dst = np.array([[0,0],[dst_size[0]-1,0],[dst_size[0]-1,dst_size[1]-1],[0,dst_size[1]-1]],dtype=\"float32\")\n",
    "            M = cv2.getPerspectiveTransform(pts,dst)\n",
    "            return cv2.warpPerspective(img,M,(dst_size[0],dst_size[1]))\n",
    "    return cv2.resize(img, dst_size)\n",
    "\n",
    "def crop_bubble(gray, cx, cy, radius=20, pad=4):\n",
    "    r = radius + pad\n",
    "    x1, y1 = max(cx-r,0), max(cy-r,0)\n",
    "    x2, y2 = min(cx+r, gray.shape[1]-1), min(cy+r, gray.shape[0]-1)\n",
    "    crop = gray[y1:y2, x1:x2]\n",
    "    if crop.size==0: return None\n",
    "    crop = cv2.resize(crop, (IMG_SIZE, IMG_SIZE))\n",
    "    return crop\n",
    "\n",
    "def detect_centers(gray):\n",
    "    detector = cv2.SimpleBlobDetector_create()\n",
    "    keypoints = detector.detect(gray)\n",
    "    centers = [(int(k.pt[0]), int(k.pt[1])) for k in keypoints]\n",
    "    return centers\n",
    "\n",
    "# ---------- LOAD ANSWER KEYS ----------\n",
    "dfA = pd.read_excel(excel_file, sheet_name=\"Set - A\")\n",
    "dfB = pd.read_excel(excel_file, sheet_name=\"Set - B\")\n",
    "\n",
    "def create_answer_dict(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    answer_dict = {}\n",
    "    topics = df.columns\n",
    "    for topic in topics:\n",
    "        for val in df[topic].dropna():\n",
    "            q_no, ans = val.replace('.', '-').split('-', 1)\n",
    "            answer_dict[int(q_no.strip())] = ans.strip().upper()\n",
    "    return answer_dict\n",
    "\n",
    "answer_A = create_answer_dict(dfA)\n",
    "answer_B = create_answer_dict(dfB)\n",
    "\n",
    "# ---------- PREPARE DATASET (Crop bubbles for CNN training) ----------\n",
    "def prepare_cnn_dataset():\n",
    "    for set_name, answer_dict in [(\"Set A\", answer_A), (\"Set B\", answer_B)]:\n",
    "        folder = os.path.join(dataset_root, set_name)\n",
    "        if not os.path.isdir(folder): continue\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if not fname.lower().endswith((\".png\",\".jpg\",\".jpeg\")): continue\n",
    "            path = os.path.join(folder, fname)\n",
    "            img = cv2.imread(path)\n",
    "            warped = warp_sheet(img)\n",
    "            gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            centers = detect_centers(gray)\n",
    "            # For training, assume left->right top->bottom order\n",
    "            centers = sorted(centers, key=lambda p: (p[1], p[0]))\n",
    "            for q_idx, (cx,cy) in enumerate(centers[:NUM_QUESTIONS]):\n",
    "                crop = crop_bubble(gray,cx,cy)\n",
    "                if crop is None: continue\n",
    "                # get label from answer dict\n",
    "                label = answer_dict.get(q_idx+1,\"BLANK\")\n",
    "                label = label.upper() if label in [\"A\",\"B\",\"C\",\"D\"] else \"BLANK\"\n",
    "                save_path = os.path.join(CROP_DIR,label,f\"{set_name}_{fname}_{q_idx}.png\")\n",
    "                cv2.imwrite(save_path, crop)\n",
    "\n",
    "# ---------- BUILD CNN ----------\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE,IMG_SIZE,1)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64,(3,3),activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(128,activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(classes),activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---------- TRAIN CNN ----------\n",
    "def train_cnn(model):\n",
    "    datagen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
    "    train_gen = datagen.flow_from_directory(CROP_DIR,target_size=(IMG_SIZE,IMG_SIZE),\n",
    "                                            color_mode='grayscale',class_mode='categorical',\n",
    "                                            subset='training',batch_size=32,shuffle=True)\n",
    "    val_gen = datagen.flow_from_directory(CROP_DIR,target_size=(IMG_SIZE,IMG_SIZE),\n",
    "                                          color_mode='grayscale',class_mode='categorical',\n",
    "                                          subset='validation',batch_size=32)\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=50, verbose=1)\n",
    "    return model\n",
    "\n",
    "# ---------- PREDICT SHEET ----------\n",
    "def predict_sheet(warped_img, model):\n",
    "    gray = cv2.cvtColor(warped_img, cv2.COLOR_BGR2GRAY)\n",
    "    centers = detect_centers(gray)\n",
    "    centers = sorted(centers, key=lambda p: (p[1], p[0]))[:NUM_QUESTIONS]\n",
    "    responses = []\n",
    "    for cx,cy in centers:\n",
    "        crop = crop_bubble(gray,cx,cy)\n",
    "        if crop is None:\n",
    "            responses.append(\"NA\")\n",
    "            continue\n",
    "        x = crop.reshape(1,IMG_SIZE,IMG_SIZE,1)/255.0\n",
    "        pred = np.argmax(model.predict(x, verbose=0))\n",
    "        responses.append(classes[pred])\n",
    "    return responses\n",
    "\n",
    "# ---------- SCORE SHEET ----------\n",
    "def score_sheet(responses, answer_dict):\n",
    "    score = 0\n",
    "    for qno, ans in answer_dict.items():\n",
    "        resp = responses[qno-1] if qno-1 < len(responses) else \"NA\"\n",
    "        if resp==ans: score+=1\n",
    "    return score\n",
    "\n",
    "# ---------- MAIN EXECUTION ----------\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Step 1: Preparing CNN dataset (cropping bubbles)...\")\n",
    "    prepare_cnn_dataset()\n",
    "    \n",
    "    print(\"Step 2: Building CNN...\")\n",
    "    model = build_cnn()\n",
    "    \n",
    "    print(\"Step 3: Training CNN...\")\n",
    "    model = train_cnn(model)\n",
    "    \n",
    "    print(\"Step 4: Predicting & scoring all sheets...\")\n",
    "    results = []\n",
    "    for set_name, answer_dict in [(\"Set A\", answer_A), (\"Set B\", answer_B)]:\n",
    "        folder = os.path.join(dataset_root, set_name)\n",
    "        if not os.path.isdir(folder): continue\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if not fname.lower().endswith((\".png\",\".jpg\",\".jpeg\")): continue\n",
    "            path = os.path.join(folder, fname)\n",
    "            img = cv2.imread(path)\n",
    "            warped = warp_sheet(img)\n",
    "            responses = predict_sheet(warped, model)\n",
    "            score = score_sheet(responses, answer_dict)\n",
    "            results.append({\"Student\": fname, \"Set\": set_name,\n",
    "                            \"Responses\":\"|\".join(responses),\"Score\":score})\n",
    "            print(f\"{fname} ({set_name}) -> score: {score}\")\n",
    "    \n",
    "    pd.DataFrame(results).to_csv(\"omr_cnn_results.csv\", index=False)\n",
    "    print(\"Saved CNN-based results to omr_cnn_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70511d7f-c3e9-46e7-a845-02df4ffa22e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing CNN dataset...\n",
      "Step 2: Building CNN...\n",
      "Step 3: Training CNN...\n",
      "Found 304 images belonging to 5 classes.\n",
      "Found 73 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.2895 - loss: 1.5234 - val_accuracy: 0.2192 - val_loss: 1.3738\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 0.3244 - loss: 1.4012 - val_accuracy: 0.3699 - val_loss: 1.3305\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - accuracy: 0.3685 - loss: 1.3733 - val_accuracy: 0.3699 - val_loss: 1.3497\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.3448 - loss: 1.3532 - val_accuracy: 0.3699 - val_loss: 1.3519\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.3480 - loss: 1.4143 - val_accuracy: 0.3699 - val_loss: 1.3473\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.3619 - loss: 1.4028 - val_accuracy: 0.3699 - val_loss: 1.3608\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.3268 - loss: 1.3503 - val_accuracy: 0.3699 - val_loss: 1.3480\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.3366 - loss: 1.3694 - val_accuracy: 0.3699 - val_loss: 1.3401\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.3476 - loss: 1.3675 - val_accuracy: 0.2466 - val_loss: 1.3633\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.3873 - loss: 1.3379 - val_accuracy: 0.3562 - val_loss: 1.3590\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.4021 - loss: 1.3265 - val_accuracy: 0.3562 - val_loss: 1.3693\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.3655 - loss: 1.3724 - val_accuracy: 0.3699 - val_loss: 1.3612\n",
      "Step 4: Predicting & scoring all sheets...\n",
      "Img1.jpeg (Set A) -> score: 0\n",
      "Img16.jpeg (Set A) -> score: 0\n",
      "Img17.jpeg (Set A) -> score: 0\n",
      "Img18.jpeg (Set A) -> score: 0\n",
      "Img19.jpeg (Set A) -> score: 15\n",
      "Img2.jpeg (Set A) -> score: 0\n",
      "Img20.jpeg (Set A) -> score: 6\n",
      "Img3.jpeg (Set A) -> score: 0\n",
      "Img4.jpeg (Set A) -> score: 0\n",
      "Img5.jpeg (Set A) -> score: 0\n",
      "Img6.jpeg (Set A) -> score: 0\n",
      "Img7.jpeg (Set A) -> score: 0\n",
      "Img8.jpeg (Set A) -> score: 41\n",
      "Img10.jpeg (Set B) -> score: 0\n",
      "Img11.jpeg (Set B) -> score: 13\n",
      "Img12.jpeg (Set B) -> score: 0\n",
      "Img13.jpeg (Set B) -> score: 3\n",
      "Img14.jpeg (Set B) -> score: 0\n",
      "Img15.jpeg (Set B) -> score: 20\n",
      "Img21.jpeg (Set B) -> score: 0\n",
      "Img22.jpeg (Set B) -> score: 0\n",
      "Img23.jpeg (Set B) -> score: 1\n",
      "Img9.jpeg (Set B) -> score: 0\n",
      "Saved CNN-based results to omr_cnn_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Robust CNN-based OMR Scoring\n",
    "# ------------------------------\n",
    "\n",
    "# REQUIREMENTS: pip install opencv-python numpy pandas tensorflow scikit-learn openpyxl matplotlib\n",
    "\n",
    "import os, cv2, numpy as np, pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "dataset_root = \".\"            \n",
    "excel_file   = \"Key (Set A and B).xlsx\"\n",
    "IMG_SIZE = 48\n",
    "CROP_DIR = \"bubble_crops\"\n",
    "NUM_QUESTIONS = 100\n",
    "classes = [\"A\",\"B\",\"C\",\"D\",\"BLANK\"]\n",
    "EPOCHS = 100  # increased epochs\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "os.makedirs(CROP_DIR, exist_ok=True)\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(CROP_DIR, cls), exist_ok=True)\n",
    "\n",
    "# ---------- HELPER FUNCTIONS ----------\n",
    "def warp_sheet(img, dst_size=(1200,1700)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    edged = cv2.Canny(blur, 50, 150)\n",
    "    cnts, _ = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:15]\n",
    "    for c in cnts:\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02*peri, True)\n",
    "        if len(approx)==4:\n",
    "            pts = approx.reshape(4,2).astype(\"float32\")\n",
    "            dst = np.array([[0,0],[dst_size[0]-1,0],[dst_size[0]-1,dst_size[1]-1],[0,dst_size[1]-1]],dtype=\"float32\")\n",
    "            M = cv2.getPerspectiveTransform(pts,dst)\n",
    "            return cv2.warpPerspective(img,M,(dst_size[0],dst_size[1]))\n",
    "    return cv2.resize(img, dst_size)\n",
    "\n",
    "def crop_bubble(gray, cx, cy, radius=20, pad=4):\n",
    "    r = radius + pad\n",
    "    x1, y1 = max(cx-r,0), max(cy-r,0)\n",
    "    x2, y2 = min(cx+r, gray.shape[1]-1), min(cy+r, gray.shape[0]-1)\n",
    "    crop = gray[y1:y2, x1:x2]\n",
    "    if crop.size==0: return None\n",
    "    crop = cv2.resize(crop, (IMG_SIZE, IMG_SIZE))\n",
    "    return crop\n",
    "\n",
    "def detect_centers(gray, visualize=False):\n",
    "    # Use blob detection with tuned parameters\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "    params.filterByArea = True\n",
    "    params.minArea = 50\n",
    "    params.maxArea = 5000\n",
    "    params.filterByCircularity = True\n",
    "    params.minCircularity = 0.3\n",
    "    detector = cv2.SimpleBlobDetector_create(params)\n",
    "    keypoints = detector.detect(gray)\n",
    "    centers = [(int(k.pt[0]), int(k.pt[1])) for k in keypoints]\n",
    "    if visualize:\n",
    "        vis = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "        for (x,y) in centers:\n",
    "            cv2.circle(vis,(x,y),10,(0,0,255),2)\n",
    "        plt.imshow(vis[:,:,::-1])\n",
    "        plt.show()\n",
    "    return centers\n",
    "\n",
    "# ---------- LOAD ANSWER KEYS ----------\n",
    "dfA = pd.read_excel(excel_file, sheet_name=\"Set - A\")\n",
    "dfB = pd.read_excel(excel_file, sheet_name=\"Set - B\")\n",
    "\n",
    "def create_answer_dict(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    answer_dict = {}\n",
    "    topics = df.columns\n",
    "    for topic in topics:\n",
    "        for val in df[topic].dropna():\n",
    "            q_no, ans = val.replace('.', '-').split('-', 1)\n",
    "            answer_dict[int(q_no.strip())] = ans.strip().upper()\n",
    "    return answer_dict\n",
    "\n",
    "answer_A = create_answer_dict(dfA)\n",
    "answer_B = create_answer_dict(dfB)\n",
    "\n",
    "# ---------- PREPARE DATASET (with augmentation) ----------\n",
    "def prepare_cnn_dataset():\n",
    "    for set_name, answer_dict in [(\"Set A\", answer_A), (\"Set B\", answer_B)]:\n",
    "        folder = os.path.join(dataset_root, set_name)\n",
    "        if not os.path.isdir(folder): continue\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if not fname.lower().endswith((\".png\",\".jpg\",\".jpeg\")): continue\n",
    "            path = os.path.join(folder, fname)\n",
    "            img = cv2.imread(path)\n",
    "            warped = warp_sheet(img)\n",
    "            gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            centers = detect_centers(gray)\n",
    "            centers = sorted(centers, key=lambda p: (p[1], p[0]))\n",
    "            for q_idx, (cx,cy) in enumerate(centers[:NUM_QUESTIONS]):\n",
    "                crop = crop_bubble(gray,cx,cy)\n",
    "                if crop is None: continue\n",
    "                label = answer_dict.get(q_idx+1,\"BLANK\")\n",
    "                label = label.upper() if label in [\"A\",\"B\",\"C\",\"D\"] else \"BLANK\"\n",
    "                save_path = os.path.join(CROP_DIR,label,f\"{set_name}_{fname}_{q_idx}.png\")\n",
    "                cv2.imwrite(save_path, crop)\n",
    "\n",
    "# ---------- BUILD CNN ----------\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE,IMG_SIZE,1)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64,(3,3),activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(128,activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(classes),activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---------- TRAIN CNN ----------\n",
    "def train_cnn(model):\n",
    "    datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 validation_split=0.2,\n",
    "                                 rotation_range=5,\n",
    "                                 width_shift_range=0.05,\n",
    "                                 height_shift_range=0.05,\n",
    "                                 brightness_range=[0.8,1.2])\n",
    "    train_gen = datagen.flow_from_directory(CROP_DIR,target_size=(IMG_SIZE,IMG_SIZE),\n",
    "                                            color_mode='grayscale',class_mode='categorical',\n",
    "                                            subset='training',batch_size=BATCH_SIZE,shuffle=True)\n",
    "    val_gen = datagen.flow_from_directory(CROP_DIR,target_size=(IMG_SIZE,IMG_SIZE),\n",
    "                                          color_mode='grayscale',class_mode='categorical',\n",
    "                                          subset='validation',batch_size=BATCH_SIZE)\n",
    "    early = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, callbacks=[early])\n",
    "    return model\n",
    "\n",
    "# ---------- PREDICT SHEET ----------\n",
    "def predict_sheet(warped_img, model, visualize=False):\n",
    "    gray = cv2.cvtColor(warped_img, cv2.COLOR_BGR2GRAY)\n",
    "    centers = detect_centers(gray, visualize=visualize)\n",
    "    centers = sorted(centers, key=lambda p: (p[1], p[0]))[:NUM_QUESTIONS]\n",
    "    responses = []\n",
    "    for cx,cy in centers:\n",
    "        crop = crop_bubble(gray,cx,cy)\n",
    "        if crop is None:\n",
    "            responses.append(\"NA\")\n",
    "            continue\n",
    "        x = crop.reshape(1,IMG_SIZE,IMG_SIZE,1)/255.0\n",
    "        pred = np.argmax(model.predict(x, verbose=0))\n",
    "        responses.append(classes[pred])\n",
    "    return responses\n",
    "\n",
    "# ---------- SCORE SHEET ----------\n",
    "def score_sheet(responses, answer_dict):\n",
    "    score = 0\n",
    "    for qno, ans in answer_dict.items():\n",
    "        resp = responses[qno-1] if qno-1 < len(responses) else \"NA\"\n",
    "        if resp==ans: score+=1\n",
    "    return score\n",
    "\n",
    "# ---------- MAIN EXECUTION ----------\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Step 1: Preparing CNN dataset...\")\n",
    "    prepare_cnn_dataset()\n",
    "    \n",
    "    print(\"Step 2: Building CNN...\")\n",
    "    model = build_cnn()\n",
    "    \n",
    "    print(\"Step 3: Training CNN...\")\n",
    "    model = train_cnn(model)\n",
    "    \n",
    "    print(\"Step 4: Predicting & scoring all sheets...\")\n",
    "    results = []\n",
    "    for set_name, answer_dict in [(\"Set A\", answer_A), (\"Set B\", answer_B)]:\n",
    "        folder = os.path.join(dataset_root, set_name)\n",
    "        if not os.path.isdir(folder): continue\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if not fname.lower().endswith((\".png\",\".jpg\",\".jpeg\")): continue\n",
    "            path = os.path.join(folder, fname)\n",
    "            img = cv2.imread(path)\n",
    "            warped = warp_sheet(img)\n",
    "            responses = predict_sheet(warped, model, visualize=False)\n",
    "            score = score_sheet(responses, answer_dict)\n",
    "            results.append({\"Student\": fname, \"Set\": set_name,\n",
    "                            \"Responses\":\"|\".join(responses),\"Score\":score})\n",
    "            print(f\"{fname} ({set_name}) -> score: {score}\")\n",
    "    \n",
    "    pd.DataFrame(results).to_csv(\"omr_cnn_results.csv\", index=False)\n",
    "    print(\"Saved CNN-based results to omr_cnn_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "370dc217-005a-4dc9-94b8-9b70220e5e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Keras model as omr_cnn_model.h5\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "model.save(\"omr_cnn_model.h5\")\n",
    "print(\"Saved Keras model as omr_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac9647a-56c2-497f-ac2a-e719566c5a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpz0a5k_fz\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpz0a5k_fz\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\HP\\AppData\\Local\\Temp\\tmpz0a5k_fz'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1891457705616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457703312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457703504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457705808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457704848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457706384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457706000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1891457707728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved TensorFlow Lite model as omr_cnn_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = tf.keras.models.load_model(\"omr_cnn_model.h5\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # optional: optimize size & latency\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite model\n",
    "with open(\"omr_cnn_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Saved TensorFlow Lite model as omr_cnn_model.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bbfbc-2416-4837-99c9-ab5d7d2b5c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
